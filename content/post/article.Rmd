---
title: "Airbnb Predictions Project"
author: "Effi Feldblum"
date: '2018-12-10'
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]

---



# Introduction

This work  was originally done for a university project. The project had two main components: a kaggle-esque competition and a full business report. Due to this dual mandate, there are numerous methods attempted that would be omitted. The competition was judged based off of plain accuracy of a held-out testing dataset. This document is a sort of summary of the finalized pipeline. I hope it is helpful to any of you out there.


This is an analysis of the factors that potentially influence Airbnb booking numbers in an attempt to predict which listings will be highly active. Highly active is defined in the binary high_booking_rate variable with 1 indicating a listing is booked more than 90% of the available time. The proportion of highly booked listings is relatively small at ~25%. There are two main goals of the analysis. First, to build a model reasonably more effective at classifying listings as high booking rate than the baseline. Second, to understand which factors are most important in identifying highly booked listings in order to provide hosts with information.


## Setup {.tabset}

### Libraries
```{r Libraries, message=FALSE, warning=FALSE}
library(DataExplorer)
library(tidyverse)
library(data.table)
library(caret)
library(DMwR)
library(purrr)
library(gridExtra)
library(h2o)
library(kableExtra)
#library(glmnet) Can be really annoying with mlr's auc metric. Not sure why or how to fix besides unloading it before using mlr.
```

### Functions used
```{r Functions}
missing_plot <- function(df){
  xx <- colSums(is.na(DF_Test))
  xx1 <- as.data.frame(xx)
  rws <- row.names(xx1)
  xx1 <- xx1 %>%
    dplyr::mutate(percent = round((xx/nrow(DF_Test))*100),4)%>%
    mutate(variable = rws)%>%
    arrange(xx)
  
  ggplot(aes(x= variable, y =xx, label = percent), data = xx1)+
    geom_col()+
    coord_flip()
  
}

find_rate_cleaning <- function(amount){
  #filter out by the amount requessted
  xx = filter(DF_Test,cleaning_fee >amount)
  #basically a table like thing
  xx=count(xx,high_booking_rate)
  return(xx[2,2]/(xx[1,2]+xx[2,2]))
}


find_rate_cleaning_opposite <- function(amount){
  #filter out by the amount requessted
  xx = filter(DF_Test,cleaning_fee <amount)
  #basically a table like thing
  xx=count(xx,high_booking_rate)
  return(xx[2,2]/(xx[1,2]+xx[2,2]))
}

```


# EDA

Let's read in the data and convert all the character variables to factors.

```{r Data Load}
DF_Test <- read.csv("data/airbnb/Airbnb_Training.csv", stringsAsFactors = FALSE,na.strings=c("","NA"))
backup <- DF_Test
DF_Test$high_booking_rate <- as.factor(DF_Test$high_booking_rate)
levels(DF_Test$high_booking_rate) <- c("Low", "High")

fact_col <- colnames(DF_Test)[sapply(DF_Test,is.character)]
for(i in fact_col)
  set(DF_Test,j=i,value = factor(DF_Test[[i]]))


glimpse(DF_Test)
```
So, there a total of `r ncol(DF_Test)` columns and  `r nrow(DF_Test)` rows. The dependent variable is high booking rate.

One of the first things noticed about the high booking rate was the relative class imbalance, with around 75% of the observations in the full dataset having a low booking rate. Booking rate is defined as high (or 1, as a dummy variable) if the listing is booked more than 90% of the available time, and low (or 0) otherwise. As we are attempting to identify and predict Airbnb listings that will have a high booking rate, there was considerable effort to understand the reasons behind this class imbalance, and combat where possible. 


```{r}
table(DF_Test$high_booking_rate)
``` 
Another quick item was that noticed was that while host_total_listing_count supposedly contains listings that a host used to have, it is identical to the host_listing_count, so it was removed.

```{r Identical vs }

# checking for identical variables
identical(DF_Test$host_total_listings_count,DF_Test$host_listings_count)

```


## First Look

Below is a condensed set of charts. As you can see, there are heavy skews and issues with deficient classes. 

```{r EDA1, warning=FALSE}
#From the DataExplorer package
plot_histogram(DF_Test)
```
These histograms show some worrying skew and potential outliers in some of our numerical variables. This was dealt with below with some identification of faulty inputs and full transformations on variables.

```{r EDA2, warning=FALSE}
plot_bar(DF_Test)
```
Some variables that will need attention here are the market, property type, bed type. All have either some abnormalities or class deficiency.

<br>
<br>


The main approach was to transform the dataset into two main variations: one more heavily manipulated and variables were reduced (Modified), and the other was kept more to the original (Lean). Then, several transformations, factor level decisions, and models were tested.


## Missing Values
```{r missing values}
# quick custom function, can be found below
missing_plot(DF_Test)

colSums(is.na(DF_Test))

```

Okay so there's a fair number  of missing values, ~ 2.5% of all observations. These missing values were split among several variables.  Treatment  was experimented with and our final pipeline contained different methods. 


### cleaning fees

The two variables with the largest proportion of missing variables were cleaning_fee and security_deposit. The main question is whether these NAs are legitimate, there on purpose, or due to a scraping or entry  error.


```{r cleaning_fees}

summary(DF_Test$cleaning_fee)
nrow(filter(DF_Test, cleaning_fee <25)) # there are 7000ish below 25
nrow(filter(DF_Test, cleaning_fee ==0)) # there are 767 at 0 (There are double the number at $10 cleaning fee)

```
The average cleaning fee is not absurd; however, the indicated skew is quite severe. Using a combination of domain knowledge and this data, it was suspected that these NAs were likely due to that listing lacking those charges. From experience, there are many listings without either a cleaning fee or a security deposit. When looking at cleaning fee, ~60% are beneath \$100, yet there are only 1.7% set at zero (there are 5% set exactly at $25). The story is much the same for security_deposit NAs. Because of this, it was decided to impute all NAs as 0. 
To take a quick peak at the skew.
```{r cleaning_fees1, message =  FALSE, warning=FALSE}
ggplot() +geom_histogram(aes(x=cleaning_fee), data = DF_Test)
#as seen above the max is 1000$ cleaning...e
#so there's def a skew here
nrow(filter(DF_Test, cleaning_fee >300)) # there are 159 listings above $300
nrow(filter(DF_Test, cleaning_fee >400)) # only 15 listings above $500

```


```{r cleaning_fee2}

c <- c(5,10,25,50,100,200,300,400)
sapply(c, find_rate_cleaning) # a custom function that finds the high booking rate of all observations above that number

sapply(c,find_rate_cleaning_opposite)# a custom function that finds the high booking rate of all observations above that number
```

What can be seen here is mirrored in many of the skewed numerical variables. There are several listings in either mansions or famous residences with prices and descriptions that indicate a high booking rate might not be necessary for their business. However, if you're looking to have a high booking rate, there appears to be an incentive to remain "sane" with your added fees. Perhaps there should  be some testing with a cleaning_fee per bedroom or per accomodates.

# Dataset Modification{.tabset}

## Description

The first tab is the setup for trying different levels for market with three different binnings attempted.
The next two tabs are used to create the bases for the two datasets referenced above [Lean, Modified].

## Market Setup

Markets
```{r fig.height=2.5, fig.width=2.5}
DF_market_18 <- DF_Test
DF_market_18$market <- as.character(DF_market_18$market)

DF_market_18$market <- ifelse(DF_market_18$market == "Nice", "Other (International)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Venice", "Other (International)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "London", "Other (International)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Agra", "Other (International)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Lagos, NG", "Other (International)",DF_market_18$market)



DF_market_18$market <- ifelse(DF_market_18$market == "San Antonio, US", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Philadelphia", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Dallas", "Other (Domestic)",DF_market_18$market)

DF_market_18$market <- ifelse(DF_market_18$market == "Dallas", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "San Antonio, US", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Indianapolis", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Providence", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Fresno", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Houston", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Fontana", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "South Florida Gulf Coast", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Fontana", "Other (Domestic)",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Pittsburg", "Other (Domestic)",DF_market_18$market)


DF_market_18$market <- ifelse(DF_market_18$market == "East Bay, CA", "San Francisco",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "South Bay, CA", "Los Angeles",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Coastal Orange County", "Los Angeles",DF_market_18$market)
DF_market_18$market <- ifelse(DF_market_18$market == "Carlsbad", "San Diego",DF_market_18$market)
DF_market_18$market <- as.factor(DF_market_18$market)


DF_market_4 <- DF_market_18
DF_market_4$market <- as.character(DF_market_4$market)

DF_market_4$market <- ifelse(DF_market_4$market == "New Orleans", "Mid City",DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Austin", "Mid City",DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "D.C.", "Mid City",DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "New Orleans", "Mid City",DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Chicago", "Mid City" ,DF_market_4$market)

DF_market_4$market <- ifelse(DF_market_4$market == "Nashville", "Mid City" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Portland", "Mid City" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Boston", "Mid City" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Denver", "Mid City" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Seattle", "Mid City" ,DF_market_4$market)


DF_market_4$market <- ifelse(DF_market_4$market == "Los Angeles", "Cali" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "San Francisco", "Cali" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "San Diego", "Cali" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Malibu", "Cali" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "Monterey Region", "Cali" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market == "North Carolina Mountains", "Other (Domestic)" ,DF_market_4$market)
DF_market_4$market <- ifelse(is.na(DF_market_4$market), "Other (Domestic)" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market== "Other (Domestic)","Other" ,DF_market_4$market)
DF_market_4$market <- ifelse(DF_market_4$market== "Other (International)","Other" ,DF_market_4$market)
DF_market_4$market <- as.factor(DF_market_4$market)

# 2 markets, NYC and everywhere else

DF_market_2 <- DF_Test
DF_market_2$market <- as.character(DF_market_2$market)

DF_market_2$market <- ifelse(DF_market_2$market == "New York", "New York","Out of Town") #send ben a screen shot

DF_market_2$market <- as.factor(DF_market_2$market)

plot_bar(DF_market_2$market)
plot_bar(DF_market_4$market)
plot_bar(DF_market_18$market)



```

Properties
```{r}

DF_prop_5 <- DF_Test

house <- c("Boat", "Chalet", "House", "Guesthouse", "Townhouse", "Vacation home", "Villa", "Tiny house", "Cottage","Bungalow" )
apartment <- c("Guest suite","apartment", "Apartment", "Loft", "Condominium","In-law","Serviced apartment","Dorm")
hotel <-  c("Resort", "Aparthotel","Bed & Breakfast", "Bed and breakfast","Boutique hotel","Hotel", "Hostel","Dorm")
nature <- c("Treehouse", "Cabin",  "Camper/RV",  "Earth House", "Earth house", "Tent", "Hut", "Yurt", "Tipi", "Nature Lodging", "Cave","Nature lodge")
other <- c("Other", "Castle", "Plane", "Barn", "Timeshare",NA)

DF_prop_5$property_type <- as.character(DF_prop_5$property_type)
DF_prop_5$property_type <- ifelse(DF_prop_5$property_type %in% house, "house",DF_prop_5$property_type)
DF_prop_5$property_type <- ifelse(DF_prop_5$property_type %in% apartment, "apartment",DF_prop_5$property_type)
DF_prop_5$property_type <- ifelse(DF_prop_5$property_type %in% hotel, "hotel",DF_prop_5$property_type)
DF_prop_5$property_type <- ifelse(DF_prop_5$property_type %in% other, "other",DF_prop_5$property_type)
DF_prop_5$property_type <- ifelse(DF_prop_5$property_type %in% nature, "nature",DF_prop_5$property_type)

DF_prop_5$property_type <- as.factor(DF_prop_5$property_type)
rm(list = c("house", "apartment", "hotel", "nature", "other"))
```

```{r echo=FALSE, fig.height=3, fig.width=3}
plot_bar(DF_market_2$market)
plot_bar(DF_market_4$market)
plot_bar(DF_market_18$market)
```

We can also take a peak at the property categories we created:

```{r echo=FALSE, fig.height=3, fig.width=3}
plot_bar(DF_prop_5$property_type)
```

Okay, based on that we're going to recombine the last three categories into other.

```{r}
DF_prop_2 <- DF_prop_5
DF_prop_2$property_type <- as.character(DF_prop_2$property_type) 
DF_prop_2$property_type <- ifelse(DF_prop_2$property_type=="hotel", "other", DF_prop_2$property_type)
DF_prop_2$property_type <- ifelse(DF_prop_2$property_type=="nature", "other", DF_prop_2$property_type)
DF_prop_2$property_type <- as.factor(DF_prop_2$property_type)
```


## Lean Dataset

```{r}

DF.lean <- DF_Test %>%
  dplyr::select(-host_total_listings_count, -amenities, -description, -host_verifications, -summary)
DF.lean$host_listings_count[DF.lean$host_listings_count==""] <- 0
DF.lean$bathrooms[is.na(DF.lean$bathrooms)] <- 0
DF.lean$beds[is.na(DF.lean$beds)] <- 0
DF.lean$bedrooms[is.na(DF.lean$bedrooms)] <- 0
DF.lean$host_has_profile_pic[is.na(DF.lean$host_has_profile_pic)] <- 'f'
DF.lean$host_identity_verified[is.na(DF.lean$host_identity_verified)] <- 'f'
DF.lean$host_is_superhost[is.na(DF.lean$host_is_superhost)] <- 'f'
DF.lean$host_listings_count[is.na(DF.lean$host_listings_count)] <- 0
DF.lean$security_deposit[is.na(DF.lean$security_deposit)] <- 0
DF.lean$cleaning_fee[is.na(DF.lean$cleaning_fee)] <- 0
DF.lean$host_response_rate[is.na(DF.lean$host_response_rate)] <- 0

DF.lean$host_response_time <- as.character(DF.lean$host_response_time)
DF.lean$host_response_time[is.na(DF.lean$host_response_time)] <- "unknown"
DF.lean$host_response_time <- as.factor(DF.lean$host_response_time)

#######WHere to add market type and property type

## DF 1 for different market types
DF.lean <- DF.lean%>% dplyr::select(-property_type, -market)
DF.lean <- cbind(DF.lean, DF_market_2$market, DF_prop_5$property_type)

DF.lean <- DF.lean%>%
  rename(market = `DF_market_2$market` )%>%
  rename(property_type = `DF_prop_5$property_type`)


DF.lean$market <- as.character(DF.lean$market)
DF.lean$market[is.na(DF.lean$market)] <- "unknown"
DF.lean$market <- as.factor(DF.lean$market)

DF.lean$property_type <- as.character(DF.lean$property_type)
DF.lean$property_type[is.na(DF.lean$property_type)] <- "unknown"
DF.lean$property_type <- as.factor(DF.lean$property_type)

DF.lean$cancellation_policy <- factor(DF.lean$cancellation_policy)
DF.lean$host_identity_verified<- factor(DF.lean$host_identity_verified)
DF.lean$host_is_superhost<- factor(DF.lean$host_is_superhost)
DF.lean$host_response_time<- factor(DF.lean$host_response_time)
DF.lean$instant_bookable<- factor(DF.lean$instant_bookable)


#only numeric variables
DF.leana <- DF.lean[ , purrr::map_lgl(DF.lean, is.numeric)]

#remove the listing id into seperate objects and delete
listing_ids <- DF.lean$Listing_ID
target_class <- DF.lean$high_booking_rate
DF.lean <- DF.lean %>%
  dplyr::select(-high_booking_rate, -Listing_ID)

#making the weird max nights above 1825 to 1825
DF.lean$maximum_nights <- ifelse(DF.lean$maximum_nights>1825, 1825, DF.lean$maximum_nights)
#making beds real or non real bed
DF.lean$bed_type <- as.character(DF.lean$bed_type)
DF.lean$bed_type <- ifelse(DF.lean$bed_type=="Real Bed", 1,0)
DF.lean$bed_type <- as.factor(DF.lean$bed_type)

# Add new variable for beds per bedroom
DF.lean$beds_per_bedroom <- ifelse(DF.lean$bedrooms != 0, DF.lean$beds/DF.lean$bedrooms, DF.lean$beds)

# Add new variables for availabilities
DF.lean$availability_3060 <- DF.lean$availability_60 - DF.lean$availability_30
DF.lean$availability_6090 <- DF.lean$availability_90 - DF.lean$availability_60

DF.lean <- DF.lean %>% dplyr::select(-availability_365, -availability_60, -availability_90,-availability_30,-latitude, -longitude)

DF.lean <- cbind(target_class, DF.lean)
DF.lean <- DF.lean %>% rename(high_booking_rate = target_class)

DF.lean$cancellation_policy <- as.character(DF.lean$cancellation_policy)
DF.lean <- DF.lean %>%
  filter(cancellation_policy != "no_refunds") #because of issues with test data
DF.lean$cancellation_policy <- as.factor(DF.lean$cancellation_policy)

levels(DF.lean$high_booking_rate) <- c("0", "1")

```


## Modified Dataset

```{r}


DF.1 <- backup %>%
  dplyr::select(-host_total_listings_count,-market)

DF.1$security_deposit[is.na(DF.1$security_deposit)] <- 0
DF.1$cleaning_fee[is.na(DF.1$cleaning_fee)] <- 0


DF_categ <- DF.1[ , purrr::map_lgl(DF.1, is.character)]
rwww <- colnames(DF_categ)
DF_categ <- DF_categ %>% dplyr::select(-summary, -description)
DF_categ[sapply(DF_categ, is.character)] <- lapply(DF_categ[sapply(DF_categ, is.character)], 
                                                   as.factor)

DF.2 <- dplyr::select(DF.1 , -rwww)
DF.1 <- cbind(DF.2,DF_categ)


# Add new variable for beds per bedroom
DF.1$beds_per_bedroom <- ifelse(DF.1$bedrooms != 0, DF.1$beds/DF.1$bedrooms, DF.1$beds)

#Might want to comment this out # Add new variables for availabilities    
 DF.1$availability_3060 <- DF.1$availability_60 - DF.1$availability_30
 DF.1$availability_6090 <- DF.1$availability_90 - DF.1$availability_60
 DF.1 <- DF.1 %>%
   dplyr::select(-availability_30, -availability_365, -availability_60,-availability_90)


DF.1 <- DF.1 %>%
  dplyr::select(-latitude, -longitude, - amenities, -Listing_ID,-host_verifications,-property_type)


#######WHere to add market type and property type

## DF 1 for different market types
DF.1.m2 <- cbind(DF.1, DF_market_2$market, DF_prop_5$property_type)

DF.1.m2 <- DF.1.m2%>%
  rename(market = `DF_market_2$market` )%>%
  rename(property_type = `DF_prop_5$property_type`)

DF.1.m2$high_booking_rate <- factor(DF.1.m2$high_booking_rate)

#adding levels of NA
DF.1.m2$host_response_time <-addNA(DF.1.m2$host_response_time)


DF.1.m2$host_response_rate[is.na(DF.1.m2$host_response_rate)] <- 0

DF.1.m2$cancellation_policy <- as.character(DF.1.m2$cancellation_policy)
DF.1.m2$cancellation_policy <- ifelse(DF.1.m2$cancellation_policy == "no_refunds","super_strict_60",DF.1.m2$cancellation_policy)
DF.1.m2$cancellation_policy <- as.factor(DF.1.m2$cancellation_policy)


#basic KNN Imputation

knnOutput <- DMwR::knnImputation(DF.1.m2[, !names(DF.1.m2) %in% "high_booking_rate"])  # perform knn imputation.

DF.1.m2 <- cbind(DF.1.m2$high_booking_rate, knnOutput)

DF.1.m2 <- rename(DF.1.m2, high_booking_rate=`DF.1.m2$high_booking_rate`)

```



# Finding Best Datasets
So, the first thing done was to discover which transformations worked the best. Both datasets had the following transformations created:  
  * Center  
  * Scale  
  * Center & Scale  
  * BoxCox  
  * BoxCox & Center & Scale  
  * Yeo Johnson


Then, each dataset was run through an ordinary logistical regression and two random forests. Both random forests ran with 5 folds. The first balanced classes and used a modulo fold assignment while the second does a pure stratified sampling.

The code and results are below.

## Operation


Complete the transformation
```{r}
foo <- DF.lean

target_class_train <- foo$high_booking_rate
foo <- foo %>% select(-high_booking_rate)

preProcValues <- preProcess(foo, method= c("YeoJohnson"))
trainTransformed <- predict(preProcValues, foo)

#recomb
foo <- cbind(target_class_train,trainTransformed)
foo <- foo %>% rename(high_booking_rate = target_class_train)

```

The logistical regression.
```{r eval = FALSE}
datafr <- DF.lean

set.seed(9444)
trainIndex <- createDataPartition(datafr$high_booking_rate, p = .8,
                                  list = FALSE, 
                                  times = 1)
training <- datafr[trainIndex,]
validation <- datafr[-trainIndex,]


full_log <- glm(high_booking_rate~., data=training, family="binomial")
full_log_preds <- predict(full_log, newdata=validation, type="response")
full_log_class <- ifelse(full_log_preds>0.5,1,0)

table.lean <- table(validation$high_booking_rate, full_log_class)
confusionMatrix(table.lean)
```

The two random forests.
```{r message=FALSE, eval = FALSE, eval =FALSE}

localH2O <- h2o.init(nthreads = -1)
y <-  1 #that is the number it is, likely 1 for me
x <- c(2:30)

datafr.h20 <- as.h2o(datafr)

nfolds=5
splits <- h2o.splitFrame(data = datafr.h20, 
                         ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks
                         seed = 1)  #setting a seed will guarantee reproducibility

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]
my_rf_mod <- h2o.randomForest(x = x,
                              y = y,
                              training_frame = train,
                              nfolds = nfolds,
                              balance_classes = TRUE,
                              fold_assignment = "Modulo",
                              keep_cross_validation_predictions = TRUE,
                              seed = 1)

my_rf_strat <- h2o.randomForest(x = x,
                                y = y,
                                training_frame = train,
                                nfolds = nfolds,
                                fold_assignment = "Stratified",
                                keep_cross_validation_predictions = TRUE,
                                seed = 1)



h2o.performance(my_rf_mod,test)
h2o.performance(my_rf_strat,test)

h2o.shutdown(prompt = TRUE)
```



The results:

```{r results1, echo = FALSE}
results1 <- data.frame(
  model = c("Modified","Modified Center" , "Modified Scale",
    "Modified Center Scale", "Modified BoxCox", "Modified Center Scale Box",
    "Lean", "Lean Center", "Lean Scale",  
    "Lean BoxCox", "Lean Scale center Boxcox","Lean YeoJohnson","Modified YeoJohnson"),
  
  logit_results = c(0.7841,0.7841,0.7841,0.7841,0.7867, 0.7867, 0.7885, 0.7885,
                    0.7885, 0.7888, 0.7888,0.7888, 0.7872),
  rf_modulo = c(0.7896084, 0.7841833, 0.8010347, 0.798494, 0.7948263, 0.7960089, 0.795861,
    0.7994087,  0.7878788, 0.7920177, 0.798374, 0.7900961, 0.8029564),
  rf_stratified =c(0.7924612, 0.7738359, 0.7841833, 0.7920177, 0.7945307, 0.7825573, 0.795122,
    0.7824095, 0.7955654, 0.8005913, 0.8002956, 0.8097561, 0.7899483))




results1 %>%
  kable() %>%
  kable_styling()
```

next step is determining which market and property levels to use.

## market level determination
Now we'll determine which of the market determinations to use. It was quickly seen that the property type with 5 categories was yielding better results, in terms of accuracy. The 12 best datasets above were varied with the three different market level types and tested the the same way as above. 

```{r  echo = FALSE}

results_market <- data.frame(
  model = c("Lean", "Lean BoxCox", "Lean YeoJohnson", "Modified", 
            "Modified Scale",  "Modified YeoJohnson","Lean", "Lean BoxCox", "Lean YeoJohnson", "Modified", 
            "Modified Scale",  "Modified YeoJohnson", "Lean", "Lean BoxCox",
            "Lean YeoJohnson", "Modified", "Modified Scale",  "Modified YeoJohnson"),
  market_type = c(2,2,2,2,2,2,4,4,4,4,4,4,18,18,18,18,18,18),
  logit_results = c(0.7896,  0.791, 0.7929, 0.7858, 0.7858,  0.7893
                    ,0.7892, 0.7924, 0.7919, 0.7843, 0.7843, 0.7875,
                    0.7896, 0.791,  0.7929,0.7858, 0.7858, 0.7893   ),
  rf_modulo = c(0.8, 0.7974871, 0.8031042, 0.8094605,  0.8010347, 0.8029564
                ,0.7963045, 0.8001478, 0.804878, 0.7821138, 0.8022173, 0.7980783,
                0.8,  0.7974871, 0.8031042, 0.8094605,  0.8022173, 0.7980783  ),
  rf_stratified =c(0.8088692, 0.807391, 0.8031042, 0.8094605, 0.8022173, 0.7980783
            ,0.7918699, 0.7831486, 0.7920177, 0.7920177, 0.7927568,  0.8004435,
                   0.8088692, 0.807391, 0.7961567,  0.7982262, 0.7881744, 0.7974871  )
)
results_market %>%
  kable() %>%
  kable_styling()
```


# Modeling on the best 4 datasets{.tabset}
## Summary of Modeling
This was, admittely, a bit of throwing different models at the training data.Being for a class, there was an attempt to demonstrate and ability to code all of these. Furthermore, it being organized like a kaggle competition with points on the line, why not give it a try.

Here are the results, code can be seen in the other tab.


```{r echo=FALSE}

model_results <- data.frame(
  datasets = c("Modified 18 Markets", "Lean 18 Markets", "Lean 2 Markets YeoJohnson", "Modified 4 markets YeoJohnson"),
  ridge_cv = c(0.7818994, 0.7822627, 0.7912786,  0.7912786),
  lasso_cv = c( 0.7890783, 0.7892643, 0.7909675,0.7925233),
  LDA =c( 0.7712, 0.7764, 0.7888, 0.7888),
  `SVM Linear`=c(0.7846,0.7912, 0.7912, 0.7906 ),
  `SVM Radial` = c(0.81, 0.8105,0.8157, 0.8161),
  `Gradient Boosting Machine` = c(0.8097561,0.8116778, 0.8031042,0.8121212),
  Xgboost =c( 0.83,0.8248, 0.8226, 0.8273 ))

model_results %>%
  kable() %>%
  kable_styling()

```

## Code for Modeling

All data was partitioned into 80% training data and 20% validation data. This was deemed a reasonable split given both the amount of data and the relative size of the testing data (the validation contains ~8000 with testing at ~7000). Except for the GBMs which used an explicit validation dataset in tuning.


FOr both Ridge and Lasso, the model is completed on the entire dataset in a matrix form. This is done here.

```{r eval = FALSE}

dfram <- DF.1.m4.YeoJohnson #swapped through datasets here

allData <- model.matrix (~ .-1, dfram[,c(1:30)])
allData_X <- model.matrix( ~ .-1, dfram[,c(2:30)])

```

Ridge Regression
```{r  eval = FALSE}

ridge = cv.glmnet(as.matrix(allData_X), dfram$high_booking_rate, family="binomial", alpha=0)
best.lambda=ridge$lambda.min
ridge_probs = predict(ridge, s=best.lambda, newx=allData_X, type="response")
ridge_class = ifelse(ridge_probs>0.45,1,0) #Used a 45% cutoff

## Calculate accuracy:
sum(ifelse(ridge_class==dfram$high_booking_rate,1,0))/nrow(dfram)
```

Lasso 
```{r eval = FALSE}
lasso = cv.glmnet(as.matrix(allData_X), dfram$high_booking_rate, family="binomial", alpha=1)
best.lambda2=lasso$lambda.min
lasso_probs = predict(lasso, s=best.lambda2, newx=allData_X, type="response")
lasso_class = ifelse(lasso_probs>0.5,1,0)

## Calculate accuracy:
sum(ifelse(lasso_class==dfram$high_booking_rate,1,0))/nrow(dfram)

```

Linear Discriminant Analysis
```{r eval = FALSE}

lda.fit <- lda(high_booking_rate~., data = training)

lda.pred <- predict(lda.fit, validation)
lda.class <- lda.pred$class
lda.pred <- lda.pred$posterior[,2]

log1 <- table(lda.class, validation$high_booking_rate)
confusionMatrix(log1)
```

Support Vector Machines: Linear and Radial Kernels
```{r eval = FALSE}

set.seed(9444)
trainIndex <- createDataPartition(datafr$high_booking_rate, p = .8,
                                  list = FALSE, 
                                  times = 1)
training <- datafr[trainIndex,]
validation <- datafr[-trainIndex,]

svm_Linear <- svm(high_booking_rate~.,data = training, method="C-classification", kernel="linear")

svm.pred <- predict(svm_Linear, validation, type="prediction")
mtab<-table(svm.pred,validation$high_booking_rate)
confusionMatrix(mtab)


svm_Rad <- svm(high_booking_rate~.,data = training,method="C-classification", kernel="radial",  probability = TRUE)

svm.pred <- predict(svm_Rad, validation, type="prediction")
mtab<-table(svm.pred,validation$high_booking_rate)
confusionMatrix(mtab)

```

GBM tuning
The parameter lists were adjusted for each dataset.
This strategy and code was found online.

```{r eval = FALSE}
#datafr[,1] <- NULL
localH2O <- h2o.init(nthreads = -1)

y <-  1 #that is the number it is, likely 1 for me
x <- c(2:30)

datafr.h20 <- as.h2o(datafr)

nfolds=5
splits <- h2o.splitFrame(data = datafr.h20, 
                         ratios = c(0.7, 0.15)  #partition data into 70%, 15%, 15% chunks
                        )  
train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]



#Cartesian grid of gbm
# GBM hyperparamters
h2o.table(train[y])


gbm_params1 <- list(learn_rate = c(0.01, 0.1),
                    max_depth = c(3, 10, 1),
                    sample_rate = c(0.8, 1.0),
                    col_sample_rate = c(0.2, 0.5, 1.0))

# Train and validate a grid of GBMs
gbm_grid1 <- h2o.grid("gbm", x = x, y = y,
                      grid_id = "gbm_grid1",
                      training_frame = train,
                      validation_frame = valid,
                      ntrees = 100,
                      seed = 1,
                      hyper_params = gbm_params1)

# Get the grid results, sorted by AUC
gbm_gridperf1 <- h2o.getGrid(grid_id = "gbm_grid1", 
                             sort_by = "auc", 
                             decreasing = TRUE)
print(gbm_gridperf1)
# Random Grid Search

# GBM hyperparamters
gbm_params2 <- list(learn_rate = seq(0.1, 1, 0.05),
                    max_depth = seq(3, 25, 1),
                    sample_rate = seq(0.5, 1.0, 0.1),
                    col_sample_rate = seq(0.1, 1.0, 0.1))
search_criteria2 <- list(strategy = "RandomDiscrete", 
                         max_models = 60)

# Train and validate a grid of GBMs
gbm_grid2 <- h2o.grid("gbm", x = x, y = y,
                      grid_id = "gbm_grid2",
                      training_frame = train,
                      validation_frame = valid,
                      ntrees = 100,
                      seed = 1,
                      hyper_params = gbm_params2,
                      search_criteria = search_criteria2)

gbm_gridperf2 <- h2o.getGrid(grid_id = "gbm_grid2", 
                             sort_by = "auc", 
                             decreasing = TRUE)
print(gbm_gridperf2)

###


gbm_params <- list(learn_rate = seq(0.05, 1, 0.01),  #updated
                   max_depth = seq(3, 13, 1),
                   sample_rate = seq(0.6, 1.0, 0.1),  #updated
                   col_sample_rate = seq(0.1, 0.8, 0.1))

search_criteria <- list(strategy = "RandomDiscrete", 
                        max_runtime_secs = 480)  #updated

gbm_grid <- h2o.grid("gbm", x = x, y = y,
                     grid_id = "gbm_grid4",
                     training_frame = train,
                     validation_frame = test,
                     ntrees = 250,
                     seed = 1,
                     hyper_params = gbm_params,
                     distribution ="bernoulli",
                     search_criteria = search_criteria)

gbm_gridperf <- h2o.getGrid(grid_id = "gbm_grid4", 
                            sort_by = "auc", 
                            decreasing = TRUE)
print(gbm_gridperf)

best_gbm_model_id <- gbm_gridperf1@model_ids[[1]]
best_gbm <- h2o.getModel(best_gbm_model_id)

#Test set prediction
best_gbm_perf <- h2o.performance(model = best_gbm, 
                                 newdata = test)
h2o.confusionMatrix(best_gbm_perf)  


h2o.shutdown(prompt = TRUE)
```


Xgboost
```{r eval = FALSE}

#create tasks
setDF(train)
setDF(test)
traintask <- makeClassifTask(data = train,target = "high_booking_rate")
testtask <- makeClassifTask(data = test,target = "high_booking_rate")


#do one hot encoding
traintask <- createDummyFeatures(obj = traintask)
testtask <- createDummyFeatures(obj = testtask)

lrn <- makeLearner("classif.xgboost",predict.type = "prob")
lrn$par.vals <- list(
  objective="binary:logistic",
  eval_metric="error",
  nrounds=200L,
  eta=0.1
)

#set parameter space
params <- makeParamSet(
  makeDiscreteParam("booster",values = c("gbtree","gblinear")),
  makeIntegerParam("max_depth",lower = 3L,upper = 25L),
  makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
  makeNumericParam("subsample",lower = 0.5,upper = 1),
  makeNumericParam("colsample_bytree",lower = 0.5,upper = 1)
)

#set resampling strategy
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)

#search strategy
ctrl <- makeTuneControlRandom(maxit = 5L, tune.threshold = TRUE)


#set parallel backend
library(parallel)
library(parallelMap)
parallelStartSocket(cpus = 5)

tic()
#parameter tuning
mytune1 <- tuneParams(learner = lrn
                     ,task = traintask
                     ,resampling = rdesc
                     ,measures = auc
                     ,par.set = params
                     ,control = ctrl
                     ,show.info = T)

toc()
#set hyperparameters
lrn_tune1 <- setHyperPars(lrn,par.vals = mytune1$x)

#train model
xgmodel1 <- mlr::train(learner = lrn_tune1,task = traintask)

#predict model
xgpred1 <- predict(xgmodel1,testtask)
confusionMatrix(xgpred1$data$response,xgpred1$data$truth)
```

# Conclusions
Overall, Xgboost was by far the best model, regardless of dataset. Submitting the best resulted in a validation accuracy of ~83% and got 83.21% on the held-out testing dataset. This was only 0.0035 percent off of the top submission.

# Trying SMOTE
To combat this, we tried several re-sampling techniques. The first was on the actual data where we performed SMOTE  (Synthetic Minority Over-sampling Technique) on the datasets. SMOTE takes the minority class and creates brand new observations through a KNN procedure. We used the default of 5 nearest neighbors. It is important to notice that SMOTE was only performed on the training set of the data and not the validation set. This was to ensure that validation results would simulate the results on true testing data.

SMOTE was tested on our best dataset with the best model (Modified 4 markets Yeo Johnson, Xgboost). It did not lead to any improvement. However, it did result in a slight improved parity in errors between the classes. Overall, it was not helpful in this scenario.

```{r eval = FALSE}
datafr <- DF.1.m2

names(datafr)
datafr[,1] <- NULL

set.seed(9444)
trainIndex <- createDataPartition(datafr$high_booking_rate, p = .8,
                                  list = FALSE, 
                                  times = 1)
train <- datafr[trainIndex,]
test <- datafr[-trainIndex,]

#create tasks
setDF(train)
setDF(test)
traintask <- makeClassifTask(data = train,target = "high_booking_rate")
testtask <- makeClassifTask(data = test,target = "high_booking_rate")

##
##What's DIFFERENT
task.smote = smote(traintask, rate = 3, nn = 5)

testtask <- createDummyFeatures(obj = testtask)
train.smote <- createDummyFeatures(obj = task.smote)
####


lrn <- makeLearner("classif.xgboost",predict.type = "prob")
lrn$par.vals <- list(
  objective="binary:logistic",
  eval_metric="error",
  nrounds=200L,
  eta=0.1
)

#set parameter space
params <- makeParamSet(
  makeDiscreteParam("booster",values = c("gbtree","gblinear")),
  makeIntegerParam("max_depth",lower = 3L,upper = 25L),
  makeNumericParam("min_child_weight",lower = 1L,upper = 10L),
  makeNumericParam("subsample",lower = 0.5,upper = 1),
  makeNumericParam("colsample_bytree",lower = 0.5,upper = 1)
)

#set resampling strategy
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)

#search strategy
ctrl <- makeTuneControlRandom(maxit = 5L, tune.threshold = TRUE)


#set parallel backend
#library(parallel)
#library(parallelMap)
parallelStartSocket(cpus = 5)

tic()
#parameter tuning
mytune1 <- tuneParams(learner = lrn
                     ,task = train.smote
                     ,resampling = rdesc
                     ,measures = auc
                     ,par.set = params
                     ,control = ctrl
                     ,show.info = T)

toc()
#set hyperparameters
lrn_tune1 <- setHyperPars(lrn,par.vals = mytune1$x)

#train model
xgmodel1 <- mlr::train(learner = lrn_tune1,task = train.smote)

#predict model
xgpred1 <- predict(xgmodel1,testtask)
confusionMatrix(xgpred1$data$response,xgpred1$data$truth)
```


### Addendum
NLP: Within NLP I also attempted several methods on both the amenities and description variables. On the description variable I used the text2vec, tm, and tidytext packages to create vocabularies, prune unused words from the vocab, and remove stopwords. Then, I tried two different sentiment analysis dictionaries, afinn and and nrc. Afinn gives a relative score of positive/negative, while nrc only classifies the word but into several emotions rather than only positive/negative. Neither set of these added variables added significant predictive poIr. The next thing tried was LDA (Latent Dirichlet Allocation), which is a type of topic modeling that uses the text to extract potential clusters prespecified. This method seemed to work best with three topics (they also seemed to be meaningful clusters); hoIver, even at its most effective, it did not add predictive poIr.

The last thing attempted was creating a non-pruned vocab of the amenities and splitting them into dummy variables I felt might be significant. It is possible that this might lead to significant improvement, but would require more analysis into individual variables. When all added variables that I believed would be “significant” (e.g. wifi, breakfast included, park) there was not much improvement, but some of the variables appeared to be significant.

A lot of this was attempted early on in the project and I'd like to come back and try it with some of the transformed datasets found above.


